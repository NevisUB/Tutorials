{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include out favorite packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Magic function for matplotlib to display inside juypter notebook\n",
    "%matplotlib inline \n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['font.size']  = 16\n",
    "matplotlib.rcParams['font.family']= 'serif'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the more useful tools for physics analysis with Pandas is so called \"query\". This is sort of like TTree->Draw on <b>steroids</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For deep learning, in particular the Faster-RCNN detector, we want to analyze the network inferences while putting restrictions on certain quantities of interest which include network score, and IoU. \n",
    "\n",
    "Pandas is perfect for slicing and dicing your data to find out what the hell is going on. No longer will you have to write, and re-write scripts to do analysis. We can load data in memory once and get our hands <b>dirty</b> until we find an interesting result. Once again I give a shout out to the Pandas book for more amazing things you can do with pandas.\n",
    "\n",
    "Lets start with a data file produced from the Faster-RCNN detector network. I've sliced off ~10MB from the ~100MB full network output to save you from downloading a very large data file.\n",
    "\n",
    "http://www.nevis.columbia.edu/~vgenty/public/some_detections.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#What is in here... just pure ascii\n",
    "!head some_detections.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "detect_df = pd.read_csv(\"some_detections.txt\", # the file\n",
    "                        sep=' ',               # separator is a space\n",
    "                        names=['d1','image_id','prob','x1','y1','x2','y2'],  # The first column is a dummy index,\n",
    "                        usecols=range(1,7))    # so lets ignore that first dummy column\n",
    "\n",
    "# Print it and take a look\n",
    "detect_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's in this file? \n",
    "\n",
    "0) The first column is the image number (it's really the TTree index number in the LArCV ROOT file...). You see this number repeats itself many times. That's b/c each row is a detection and we get multiple detections per image.\n",
    "\n",
    "1) Second column is the detection score.\n",
    "\n",
    "2) Columns [3,4,5,6] are the four corners of a bounding box x1,y1,x2,y2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ground truth label is this specific output so we will have to add it in by hand. All images with less than 1000 are \"cosmic\" images and contain no MC neutrino. Image larger than that actually have a neutrino in them. Trust me on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How many cosmic inferences do we have?\n",
    "print detect_df.query('image_id <= 1000').index.size\n",
    "\n",
    "# How many neutrino inferences do we have?\n",
    "print detect_df.query('image_id > 1000').index.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very useful since the detections are not unique but lets go ahead and assign a ground truth class to each of the images for this we can use pandas.apply() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make a function that assigs the ground truth and takes as input the row\n",
    "def assign_gt(row):\n",
    "    if row['image_id'] <= 1000 :\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "# make a new column called gt (ground truth)\n",
    "detect_df['gt'] = detect_df.apply(assign_gt,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually we care about the <b>best</b> detection per image, so lets slice the dataframe <b>per image</b> and pick the box with the highest network score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_predictions = []\n",
    "for name, group in detect_df.groupby('image_id'):\n",
    "    # sort the group bases on probability\n",
    "    sorted_df = group.sort_values(by='prob',ascending=False)\n",
    "    \n",
    "    # get the top index (use positional indexer iloc, not ix)\n",
    "    # there is a more elegant way to do this but you will find when doing analysis\n",
    "    # just do what you have to to get the job done\n",
    "    top_predictions.append(sorted_df.iloc[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Convert to a data frame\n",
    "top_df = pd.DataFrame(top_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Just want to make a note here that linespace and arange are not the same. You should provide 1 extra bin\n",
    "# in the case of linspace (40->41) to get the correct binning. In arange you see I go over the li\n",
    "\n",
    "print \"np.lnspace\"\n",
    "print np.linspace(0,1.0,40)\n",
    "\n",
    "print \"np.arange\"\n",
    "print np.arange(0,1.0+0.025,0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arange_   = np.arange(0,1.0,0.025)\n",
    "linspace_ = np.linspace(0,1.0,40)\n",
    "bins_     = np.arange(0,1.0+0.025,0.025)\n",
    "\n",
    "\n",
    "l = plt.hist(arange_,bins=linspace_)\n",
    "print \"39 bins...\"\n",
    "plt.show()\n",
    "\n",
    "a = plt.hist(arange_,bins_)\n",
    "print \"40 nice bins...\"\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lets now look at how well the detection network can separation neutrinos from cosmic images\n",
    "\n",
    "neutrinos = top_df.query('gt == 1.0')['prob'].values\n",
    "cosmics   = top_df.query('gt == 0.0')['prob'].values\n",
    "\n",
    "\n",
    "div=0.025\n",
    "bins = np.arange(0,1.0+div,div)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "\n",
    "# note... normed=True here will give an area normalized histogram, not what we want\n",
    "# if we scale cosmic and neutrino events up all we have to do is multiply a scalar\n",
    "# by this histogram to get the total rate, not the case for area normalized!\n",
    "ax.hist(neutrinos,\n",
    "        bins=bins,\n",
    "        weights=np.array([1/float(neutrinos.size) for _ in xrange(neutrinos.size)]),\n",
    "        alpha=0.5,\n",
    "        lw=2,\n",
    "        histtype='stepfilled',\n",
    "        color='blue',\n",
    "       label='neutrino')\n",
    "ax.hist(cosmics,\n",
    "        bins=bins,\n",
    "        weights=np.array([1/float(cosmics.size) for _ in xrange(cosmics.size)]),\n",
    "        alpha=0.5,\n",
    "        lw=2,\n",
    "        histtype='stepfilled',\n",
    "        color='red',\n",
    "        label='cosmics')\n",
    "\n",
    "ax.set_ylim(0,.10)\n",
    "ax.set_ylabel(\"Events (Normalized)\",fontweight='bold')\n",
    "ax.set_xlabel(\"Nu Score\",fontweight='bold')\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "#Kaleko's suggestion\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
